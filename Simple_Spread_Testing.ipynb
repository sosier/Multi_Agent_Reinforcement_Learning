{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Spread Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Simple Spread (Collaborative Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent observations: `[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, communication]`\n",
    " - `self_vel = (2, )`\n",
    " - `self_pos = (2, )`\n",
    " - `landmark_rel_positions = (2 * N, )`\n",
    " - `other_agent_rel_positions = (2 * (N - 1), )`\n",
    " - `communication = (2 * (N - 1), )`\n",
    "\n",
    "Agent action space: `[no_action, move_left, move_right, move_down, move_up] = (0-4)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_spread_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_spread_v3.parallel_env(N=5)\n",
    "observations, infos = env.reset()\n",
    "observations, infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations[\"agent_0\"].shape)\n",
    "observations[\"agent_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space(\"agent_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where you would insert your policy\n",
    "# actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "actions = {agent: 0 for agent in env.agents}\n",
    "observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "observations, rewards, terminations, truncations, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Variant (Custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent observations: `[self_is_adversary, self_vel, self_pos, landmark_rel_positions, other_agent_is_adversary_rel_positions]`\n",
    " - `self_is_adversary = (1, )`: 0 / 1 flag\n",
    " - `self_vel = (2, )`\n",
    " - `self_pos = (2, )`\n",
    " - `landmark_rel_positions = (2 * n_landmarks, )`\n",
    " - `other_agent_is_adversary_rel_positions = ((1 + 2) * (n_agents + n_adversaries - 1), )`: 0 / 1 flag  for if that other agent is an adversary + relative position for the other agent times the number of other agents\n",
    "\n",
    "Agent action space: `[no_action, move_left, move_right, move_down, move_up] = (0-4)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import simple_spread_adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_spread_adversarial.parallel_env(n_agents=2, n_adversaries=2, n_landmarks=2)\n",
    "observations, infos = env.reset()\n",
    "observations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_agents, env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations[\"agent_0\"].shape)\n",
    "observations[\"agent_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space(\"agent_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where you would insert your policy\n",
    "# actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "observations, rewards, terminations, truncations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full episode\n",
    "env = simple_spread_adversarial.parallel_env(\n",
    "    n_agents=2,\n",
    "    n_adversaries=2,\n",
    "    n_landmarks=3,\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "observations, infos = env.reset()\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 3\n",
    "env = simple_spread_v3.parallel_env(N=temp)\n",
    "observations, infos = env.reset()\n",
    "observations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in env.agents:\n",
    "    observation = observations[agent]\n",
    "    \n",
    "    self_vel = observation[:2]\n",
    "    self_pos = observation[2:4]\n",
    "    idx = 4 + temp * 2\n",
    "    landmark = observation[4:idx]\n",
    "    idx2 = idx + (temp - 1) * 2\n",
    "    other_pos = observation[idx:idx2]\n",
    "    comms = observation[idx2:]\n",
    "    \n",
    "    print(\"self vel: \", self_vel)\n",
    "    print(\"self pos: \", self_pos)\n",
    "    print(\"landmarks: \", landmark)\n",
    "    print(\"other players: \", other_pos)\n",
    "    print(\"comms: \", comms)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def observation(self, agent, world):\n",
    "#         # get positions of all entities in this agent's reference frame\n",
    "#         entity_pos = []\n",
    "#         for entity in world.landmarks:  # world.entities:\n",
    "#             entity_pos.append(entity.state.p_pos - agent.state.p_pos)\n",
    "#         # communication of all other agents\n",
    "#         comm = []\n",
    "#         other_pos = []\n",
    "#         for other in world.agents:\n",
    "#             if other is agent:\n",
    "#                 continue\n",
    "#             comm.append(other.state.c)\n",
    "#             other_pos.append(other.state.p_pos - agent.state.p_pos)\n",
    "#         return np.concatenate(\n",
    "#             [agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + comm\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts: Vary the amount of comm being transferred. By default, other pos are included outside of the comm vector. Potential Baselines: Mask landmarks / Mask other pos. Masking both doesn't make much sense as it essentially. becomes. Run on small number of iterations to learn policy. Ideas for custom defined comm vector: provide velocity of self to other agents (2 per other agent, 2N-1 like right now). alternatively, provide euclidan distance to each of the landmarks (my thinking is that it would explicitly force the agents to learn instead of learning implicitly via the reward func. The number would be N per agent). This could either be an absolute L2 distance or some binary variable. The binary variable could either be N per other agent (1 if within some parameter bound to landmark x, 0 if not) or 2 per other agent (1 if within some parameter bound to any landmark, 0 if not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communciation (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (simple_spread_comms.py, line 150)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3460\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\n\u001b[0;31m    import simple_spread_comms\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Desktop/final/Multi_Agent_Reinforcement_Learning/simple_spread_comms.py:150\u001b[0;36m\u001b[0m\n\u001b[0;31m    comm.append([1,1])\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import simple_spread_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent_0': array([ 0.        ,  0.        , -0.22761886,  0.9517223 , -0.6603963 ,\n",
       "         -1.492733  , -0.7116871 , -0.9740574 ,  1.0357677 , -0.30283958,\n",
       "          0.7488561 , -0.714137  ,  1.0947871 , -0.4342686 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ], dtype=float32),\n",
       "  'agent_1': array([ 0.        ,  0.        ,  0.52123725,  0.23758529, -1.4092524 ,\n",
       "         -0.77859604, -1.4605433 , -0.25992033,  0.2869115 ,  0.41129747,\n",
       "         -0.7488561 ,  0.714137  ,  0.345931  ,  0.27986842,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ], dtype=float32),\n",
       "  'agent_2': array([ 0.        ,  0.        ,  0.86716825,  0.51745373, -1.7551835 ,\n",
       "         -1.0584644 , -1.8064742 , -0.5397888 , -0.0590195 ,  0.13142903,\n",
       "         -1.0947871 ,  0.4342686 , -0.345931  , -0.27986842,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ], dtype=float32)},\n",
       " {'agent_0': {}, 'agent_1': {}, 'agent_2': {}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 3\n",
    "env = simple_spread_comms.parallel_env(N = temp)\n",
    "observations, infos = env.reset()\n",
    "observations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self vel:  [0. 0.]\n",
      "self pos:  [-0.22761886  0.9517223 ]\n",
      "landmarks:  [-0.6603963  -1.492733   -0.7116871  -0.9740574   1.0357677  -0.30283958]\n",
      "other players:  [ 0.7488561 -0.714137   1.0947871 -0.4342686]\n",
      "comms:  [0. 0. 0. 0.]\n",
      "\n",
      "self vel:  [0. 0.]\n",
      "self pos:  [0.52123725 0.23758529]\n",
      "landmarks:  [-1.4092524  -0.77859604 -1.4605433  -0.25992033  0.2869115   0.41129747]\n",
      "other players:  [-0.7488561   0.714137    0.345931    0.27986842]\n",
      "comms:  [0. 0. 0. 0.]\n",
      "\n",
      "self vel:  [0. 0.]\n",
      "self pos:  [0.86716825 0.51745373]\n",
      "landmarks:  [-1.7551835  -1.0584644  -1.8064742  -0.5397888  -0.0590195   0.13142903]\n",
      "other players:  [-1.0947871   0.4342686  -0.345931   -0.27986842]\n",
      "comms:  [0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for agent in env.agents:\n",
    "    observation = observations[agent]\n",
    "    \n",
    "    self_vel = observation[:2]\n",
    "    self_pos = observation[2:4]\n",
    "    idx = 4 + temp * 2\n",
    "    landmark = observation[4:idx]\n",
    "    idx2 = idx + (temp - 1) * 2\n",
    "    other_pos = observation[idx:idx2]\n",
    "    comms = observation[idx2:]\n",
    "    \n",
    "    print(\"self vel: \", self_vel)\n",
    "    print(\"self pos: \", self_pos)\n",
    "    print(\"landmarks: \", landmark)\n",
    "    print(\"other players: \", other_pos)\n",
    "    print(\"comms: \", comms)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
