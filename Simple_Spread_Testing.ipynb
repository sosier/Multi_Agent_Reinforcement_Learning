{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Spread Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Simple Spread (Collaborative Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent observations: `[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, communication]`\n",
    " - `self_vel = (2, )`\n",
    " - `self_pos = (2, )`\n",
    " - `landmark_rel_positions = (2 * N, )`\n",
    " - `other_agent_rel_positions = (2 * (N - 1), )`\n",
    " - `communication = (2 * (N - 1), )`\n",
    "\n",
    "Agent action space: `[no_action, move_left, move_right, move_down, move_up] = (0-4)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_spread_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_spread_v3.parallel_env(N=5)\n",
    "observations, infos = env.reset()\n",
    "observations, infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations[\"agent_0\"].shape)\n",
    "observations[\"agent_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space(\"agent_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where you would insert your policy\n",
    "# actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "actions = {agent: 0 for agent in env.agents}\n",
    "observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "observations, rewards, terminations, truncations, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Variant (Custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent observations: `[self_is_adversary, self_vel, self_pos, landmark_rel_positions, other_agent_is_adversary_rel_positions]`\n",
    " - `self_is_adversary = (1, )`: 0 / 1 flag\n",
    " - `self_vel = (2, )`\n",
    " - `self_pos = (2, )`\n",
    " - `landmark_rel_positions = (2 * n_landmarks, )`\n",
    " - `other_agent_is_adversary_rel_positions = ((1 + 2) * (n_agents + n_adversaries - 1), )`: 0 / 1 flag  for if that other agent is an adversary + relative position for the other agent times the number of other agents\n",
    "\n",
    "Agent action space: `[no_action, move_left, move_right, move_down, move_up] = (0-4)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import imageio\n",
    "import simple_spread_adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_spread_adversarial.parallel_env(n_agents=2, n_adversaries=2, n_landmarks=2)\n",
    "observations, infos = env.reset()\n",
    "observations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_agents, env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations[\"agent_0\"].shape)\n",
    "observations[\"agent_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space(\"agent_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where you would insert your policy\n",
    "# actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "observations, rewards, terminations, truncations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full episode\n",
    "env = simple_spread_adversarial.parallel_env(\n",
    "    n_agents=2,\n",
    "    n_adversaries=2,\n",
    "    n_landmarks=3,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "frames = []\n",
    "observations, infos = env.reset()\n",
    "frames.append(env.render())\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    frames.append(env.render())\n",
    "\n",
    "env.close()\n",
    "\n",
    "imageio.mimwrite(\n",
    "    \"rendered_episode.gif\",\n",
    "    frames,\n",
    "    loop=0  # Infinite loop gif\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 3\n",
    "env = simple_spread_v3.parallel_env(N=temp)\n",
    "observations, infos = env.reset()\n",
    "observations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in env.agents:\n",
    "    observation = observations[agent]\n",
    "    \n",
    "    self_vel = observation[:2]\n",
    "    self_pos = observation[2:4]\n",
    "    idx = 4 + temp * 2\n",
    "    landmark = observation[4:idx]\n",
    "    idx2 = idx + (temp - 1) * 2\n",
    "    other_pos = observation[idx:idx2]\n",
    "    comms = observation[idx2:]\n",
    "    \n",
    "    print(\"self vel: \", self_vel)\n",
    "    print(\"self pos: \", self_pos)\n",
    "    print(\"landmarks: \", landmark)\n",
    "    print(\"other players: \", other_pos)\n",
    "    print(\"comms: \", comms)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def observation(self, agent, world):\n",
    "#         # get positions of all entities in this agent's reference frame\n",
    "#         entity_pos = []\n",
    "#         for entity in world.landmarks:  # world.entities:\n",
    "#             entity_pos.append(entity.state.p_pos - agent.state.p_pos)\n",
    "#         # communication of all other agents\n",
    "#         comm = []\n",
    "#         other_pos = []\n",
    "#         for other in world.agents:\n",
    "#             if other is agent:\n",
    "#                 continue\n",
    "#             comm.append(other.state.c)\n",
    "#             other_pos.append(other.state.p_pos - agent.state.p_pos)\n",
    "#         return np.concatenate(\n",
    "#             [agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + comm\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts: Vary the amount of comm being transferred. By default, other pos are included outside of the comm vector. Potential Baselines: Mask landmarks / Mask other pos. Masking both doesn't make much sense as it essentially. becomes. Run on small number of iterations to learn policy. Ideas for custom defined comm vector: provide velocity of self to other agents (2 per other agent, 2N-1 like right now). alternatively, provide euclidan distance to each of the landmarks (my thinking is that it would explicitly force the agents to learn instead of learning implicitly via the reward func. The number would be N per agent). This could either be an absolute L2 distance or some binary variable. The binary variable could either be N per other agent (1 if within some parameter bound to landmark x, 0 if not) or 2 per other agent (1 if within some parameter bound to any landmark, 0 if not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communciation (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simple_spread_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent_0': array([ 0.        ,  0.        ,  0.15948704, -0.44753256, -0.16793479,\n",
       "          0.74102026,  0.5801595 , -0.16484697, -0.4447318 ,  1.1122671 ,\n",
       "         -1.0538346 ,  0.6712349 ,  0.5373433 ,  0.39537844,  0.        ,\n",
       "          0.        ], dtype=float32),\n",
       "  'agent_1': array([ 0.        ,  0.        , -0.89434755,  0.22370231,  0.8858998 ,\n",
       "          0.06978536,  1.6339941 , -0.83608186,  0.6091027 ,  0.44103223,\n",
       "          1.0538346 , -0.6712349 ,  1.5911779 , -0.27585644,  0.        ,\n",
       "          0.        ], dtype=float32),\n",
       "  'agent_2': array([ 0.        ,  0.        ,  0.6968304 , -0.05215412, -0.7052781 ,\n",
       "          0.3456418 ,  0.04281615, -0.5602254 , -0.98207515,  0.71688867,\n",
       "         -0.5373433 , -0.39537844, -1.5911779 ,  0.27585644,  0.        ,\n",
       "          0.        ], dtype=float32)},\n",
       " {'agent_0': {}, 'agent_1': {}, 'agent_2': {}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp controls number of landmarks and players and the print\n",
    "# could split it up like in the adversarial file so that landmarks and players are different numbers\n",
    "temp = 3\n",
    "# SEE BOTTOM OF .PY FILE TO SEE WHAT EACH MODE DOES 0-4\n",
    "env = simple_spread_comms.parallel_env(N = temp, comm_mode=4)\n",
    "observations, infos = env.reset()\n",
    "observations, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self vel:  [0. 0.]\n",
      "self pos:  [0.6473063  0.52806616]\n",
      "landmarks:  [-1.2742863  -1.1879019  -1.4164813  -0.7793342  -1.140535    0.41489962]\n",
      "relative pos of other players:  [-0.4060039  -0.41880074 -1.0631205  -0.43103477]\n",
      "comms:  [0. 1.]\n",
      "\n",
      "self vel:  [0. 0.]\n",
      "self pos:  [0.24130242 0.10926543]\n",
      "landmarks:  [-0.8682824  -0.7691011  -1.0104774  -0.36053345 -0.7345311   0.83370036]\n",
      "relative pos of other players:  [ 0.4060039   0.41880074 -0.65711653 -0.01223403]\n",
      "comms:  [0. 1.]\n",
      "\n",
      "self vel:  [0. 0.]\n",
      "self pos:  [-0.41581413  0.0970314 ]\n",
      "landmarks:  [-0.21116579 -0.75686705 -0.35336083 -0.3482994  -0.07741455  0.8459344 ]\n",
      "relative pos of other players:  [1.0631205  0.43103477 0.65711653 0.01223403]\n",
      "comms:  [0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for agent in env.agents:\n",
    "    observation = observations[agent]\n",
    "    \n",
    "    self_vel = observation[:2]\n",
    "    self_pos = observation[2:4]\n",
    "    idx = 4 + temp * 2\n",
    "    landmark = observation[4:idx]\n",
    "    idx2 = idx + (temp - 1) * 2\n",
    "    other_pos = observation[idx:idx2]\n",
    "    comms = observation[idx2:]\n",
    "    \n",
    "    print(\"self vel: \", self_vel)\n",
    "    print(\"self pos: \", self_pos)\n",
    "    print(\"landmarks: \", landmark)\n",
    "    print(\"relative pos of other players: \", other_pos)\n",
    "    print(\"comms: \", comms)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN PLACEHOLDER\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create Q-Network and optimizer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m env \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 14\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming all agents have the same state dimensions\u001b[39;00m\n\u001b[1;32m     15\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_0\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mn  \u001b[38;5;66;03m# Assuming all agents have the same action space\u001b[39;00m\n\u001b[1;32m     16\u001b[0m q_network \u001b[38;5;241m=\u001b[39m DQN(state_dim, action_dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "episode_max_length = 100\n",
    "\n",
    "gamma = 0.95\n",
    "learning_rate = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "reward_of_episodes = []\n",
    "for episode in range(num_episodes):\n",
    "    states = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(episode_max_length):\n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            \n",
    "        env.render(mode='human')\n",
    "\n",
    "        for agent in env.agents:\n",
    "            state = torch.FloatTensor(states[agent])\n",
    "            next_state = torch.FloatTensor(next_states[agent])\n",
    "            reward = rewards[agent]\n",
    "            done = dones[agent]\n",
    "\n",
    "            # Calculate the target Q-value\n",
    "            with torch.no_grad():\n",
    "                target_q_value = reward + gamma * torch.max(q_network(next_state))\n",
    "\n",
    "            # Calculate the current Q-value\n",
    "            current_q_value = q_network(state)[actions[agent]]\n",
    "\n",
    "            # Calculate the loss and perform optimization\n",
    "            loss = nn.functional.mse_loss(current_q_value, target_q_value)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Other possible code here\n",
    "    reward_of_episodes.append(total_reward)\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
